# Abstract

The advancement of foundation models has expanded possibilities for robotic systems. Progress in large language models (LLMs) and 3D perception models enables robots to integrate extensive world knowledge, significantly enhancing their generalization capabilities. Therefore，we propose 3D-Planner, an open-vocabulary robotic manipulation approach that establishes alignment between semantic 3D reconstruction capabilities and LLM competencies specifically for language-guided manipulation tasks. The system comprises three core components: (i) Observer，a perception module that embeds semantic information and grasp-relevant features into 3D scenes, enabling geometric, semantic, and visibility parsing; (ii) Planner，a planning module utilizing LLMs to accurately interpret human intent and perform reasoning/task planning; (iii)Executor，an action module that generates both candidate grasps and robot trajectories (dense 6-DoF waypoint sequences for end-effectors) in open-world environments. Based on this approach, we developed a robotic system for long-horizon manipulation tasks capable of handling for open-set instructions and objects. Hardware validation on ROKAE robot confirms 3D-Planner's statistically significant advantages: 1) 96% success rate in 5 low-level tasks (vs. xxx% for baselines), 2)60% higher accuracy with 25% fewer steps in high-level planning,3)X% higher average success rate in 6 staged manipulation tasks across three state-of-the-art baselines SayCan, CaP, Voxposer.Demo videos and an overview of 3D-Planner are available at https://dajiebi.github.io/3D-Planner.github.io/.